\documentclass{beamer}
%\documentclass[aspectratio=169]{beamer}
%
\mode<presentation>
{
  \usetheme{default}      
  \usecolortheme{default}
  \usefonttheme{default} 
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{bm}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\bxi}{\bm{x}^{(i)}}
\newcommand{\bx}{\bm{x}}
\newcommand{\xij}{x^{(i)}_j}
\newcommand{\yi}{y^{(i)}}
\newcommand{\yhati}{\hat{y}^{(i)}}

\title[Course presentation]{Machine learning from scratch}
\subtitle{Lecture 1: Mathematical background}
\author{Alexis Zubiolo\newline\texttt{alexis.zubiolo@gmail.com}}
\institute{Data Science Team Lead @ Adcash}
\date{January 26, 2017}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Before we start}
IT STEP will be organizing a Tech night on \textbf{February 16th} (Thursday) from 7pm. I will (probably) be giving a talk. The course will most likely be postponed.
\end{frame}

\begin{frame}{Motivation, vocabulary and notations}
In \textbf{supervised learning} tasks, we are given a \textit{data set} of the form:
$$ D = \left\{ \left(\bxi, \yi\right), \bxi \in \X, \yi \in \Y, i \in \{1, \dots, n \}  \right\}$$
\pause
\begin{itemize}
	\item $n$ is the size of the data set (number of \textit{instances}/\textit{samples})
	\item In most applications:
	\begin{itemize}
		\item $\X = \real^d$ ($d$ is the \textit{dimensionality})
		\item $\Y = \real$ (\textit{regression}) or $\Y \subset \N$ (\textit{classification})
	\end{itemize}		
	\item $\bx \in \X$ is the \textit{feature vector} and $y \in \Y$ is the \textit{label}
\end{itemize}
\end{frame}

\begin{frame}{Motivation, vocabulary and notations}
Solving a \textbf{supervised learning problem} is finding (or \textit{learning}) a function (or \textit{hypothesis}) $h : \X \mapsto \Y$ such that for $(\bx, y)\in D$, $h(\bx)$ is a \textit{good} estimation  (or approximation) of $y$.
\pause
\vfill
We often write $h(\bx) = \yhat$ ($\yhat$ is the \textit{prediction} of $\bx$ by $h$).
\pause
\vfill
This raises \textbf{2 questions}:
\begin{itemize}
	\item How to define $h$?
	\item How to assess whether $\yhat$ is a good approximation of $y$?
\end{itemize}
\end{frame}

\begin{frame}{Hypothesis parametrization}
$h$ is often defined by a parameter vector $\theta$ and can be noted $h_\theta$. \pause Several ways to parametrize $h$ exist:
\begin{itemize}
	\item \textbf{Linear model}: $h(\bx) = \theta^T \bx$
	\item \textbf{Polynomial kernel} (degree $k$): $h(\bx) = \left(1 + \theta^T \bx\right)^k$
	\item Other kernels exist, more on this when we talk about duality
	\item With a \textbf{neural net}, more on this later as well
	\item \ldots
\end{itemize}
\vfill
\pause
How you define $h$ highly depends on the application, for example:
\begin{itemize}
	\item Sometimes a lot of data preprocessing has been made and a simple model (e.g. linear) would work well
	\item You might have \textbf{time/hardware constraints}: In this case going for a too complex model might be crippling
	\item For neural net, the architecture depends a lot on the type of data you have
\end{itemize}
\end{frame}

\begin{frame}{Loss functions}
Recall we want to know whether $y$ a good prediction of $\yhat$.
\end{frame}

\begin{frame}{Regularization}

\end{frame}

\begin{frame}{Optimization}
Now, we want to minimize a function of the form
\begin{equation}
J(\theta) = \sum_{i = 1}^{n} \ell\left(\yi, \yhati\right)
\end{equation}
\end{frame}

\end{document}
