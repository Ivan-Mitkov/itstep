\documentclass{beamer}
%\documentclass[aspectratio=169]{beamer}
%
\mode<presentation>
{
  \usetheme{default}      
  \usecolortheme{default}
  \usefonttheme{default} 
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{bm}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\bxi}{\bm{x}^{(i)}}
\newcommand{\bx}{\bm{x}}
\newcommand{\xij}{x^{(i)}_j}
\newcommand{\yi}{y^{(i)}}
\newcommand{\yhati}{\hat{y}^{(i)}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title[Course presentation]{Machine learning from scratch}
\subtitle{Lecture 1: Mathematical background}
\author{Alexis Zubiolo\newline\texttt{alexis.zubiolo@gmail.com}}
\institute{Data Science Team Lead @ Adcash}
\date{January 26, 2017}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Before we start}
IT STEP will be organizing a Tech night on \textbf{February 16th} (Thursday) from 7pm. I will (probably) be giving a talk. The course will most likely be postponed.
\end{frame}

\begin{frame}{Motivation, vocabulary and notations}
In \textbf{supervised learning} tasks, we are given a \textit{data set} of the form:
$$ D = \left\{ \left(\bxi, \yi\right), \bxi \in \X, \yi \in \Y, i \in \{1, \dots, n \}  \right\}$$
\pause
\begin{itemize}
	\item $n$ is the size of the data set (number of \textit{instances}/\textit{samples})
	\item In most applications:
	\begin{itemize}
		\item $\X = \real^d$ ($d$ is the \textit{dimensionality})
		\item $\Y = \real$ (\textit{regression}) or $\Y \subset \N$ (\textit{classification})
	\end{itemize}		
	\item $\bx \in \X$ is the \textit{feature vector} and $y \in \Y$ is the \textit{label}
\end{itemize}
\end{frame}

\begin{frame}{Motivation, vocabulary and notations}
Solving a \textbf{supervised learning problem} is finding (or \textit{learning}) a function (or \textit{hypothesis}) $h : \X \mapsto \Y$ such that for $(\bx, y)\in D$, $h(\bx)$ is a \textit{good} estimation  (or approximation) of $y$.
\pause
\vfill
We often write $h(\bx) = \yhat$ ($\yhat$ is the \textit{prediction} of $\bx$ by $h$).
\pause
\vfill
This raises \textbf{2 questions}:
\begin{itemize}
	\item How to define $h$?
	\item How to assess whether $\yhat$ is a good approximation of $y$?
\end{itemize}
\end{frame}

\begin{frame}{Hypothesis parametrization}
$h$ is often defined by a parameter vector $\theta$ and can be noted $h_\theta$. \pause Several ways to parametrize $h$ exist:
\begin{itemize}
	\item \textbf{Linear model}: $h(\bx) = \theta^T \bx$
	\item \textbf{Polynomial kernel} (degree $k$): $h(\bx) = \left(1 + \theta^T \bx\right)^k$
	\item Other kernels exist, more on this when we talk about duality
	\item With a \textbf{neural net}, more on this later as well
	\item \ldots
\end{itemize}
\vfill
\pause
How you define $h$ highly depends on the application, for example:
\begin{itemize}
	\item Sometimes a lot of data preprocessing has been made and a simple model (e.g. linear) would work well
	\item You might have \textbf{time/hardware constraints}: In this case going for a too complex model might be crippling
	\item For neural net, the architecture depends a lot on the type of data you have
\end{itemize}
\end{frame}


\begin{frame}{Linear Algebra concepts (1)}
Recall the regression example from the previous lecture:
\pause
\vfill
\begin{table}
\centering
\begin{tabular}{r|r|r}
living area (m$^2$) &  \textbf{\# bedrooms} & price (1000's BGN) \\\hline
50 & \textbf{1} & 30\\
76 & \textbf{2} & 48\\
26 & \textbf{1} & 12\\
102 & \textbf{3} & 90\\
\pause
61 & \textbf{2} & ?
\end{tabular}
\end{table}
\vfill
\pause
Illustration of the introduced notations:
\begin{itemize}
	\item $\X = \real^2$ ($d = 2$ dimensions: living area and  \# bedrooms)
	\item $\Y = \real$ (regression task)
	\item $\bx^{(1)} = \left[ 50, 1 \right]^T$ and $y^{(1)} = 30000$
\end{itemize}
\end{frame}

\begin{frame}{Linear algebra concepts (2)}
\begin{table}
\centering
\begin{tabular}{r|r|r}
living area (m$^2$) &  \textbf{\# bedrooms} & price (1000's BGN) \\\hline
50 & \textbf{1} & 30\\
76 & \textbf{2} & 48\\
26 & \textbf{1} & 12\\
102 & \textbf{3} & 90\\
\end{tabular}
\end{table}
\vfill
\pause
Using a \textbf{linear regression model} gives
\begin{equation*}
h_\theta(\bx) = \theta_0 + \theta_1 x_1 + \theta_2 x_2
\end{equation*}
Generalization to any dimensionality $d$:
\begin{equation*}
h(\bx) = \sum_{j = 0}^{d} \theta_j x_j = \theta^T \bx
\end{equation*}
Here, we set $\bx_0 = 1$ so that $\theta_0$ is included in $\theta$. $\theta^T \bx$ is called the dot product (or inner product) between $theta$ and $\bx$ and is sometimes noted $\langle {\theta, \bx} \rangle$.
\end{frame}

\begin{frame}{Ordinary least squares}
\begin{table}
\centering
\begin{tabular}{r|r|r}
living area (m$^2$) &  \textbf{\# bedrooms} & price (1000's BGN) \\\hline
50 & \textbf{1} & 30\\
76 & \textbf{2} & 48\\
26 & \textbf{1} & 12\\
102 & \textbf{3} & 90\\
\end{tabular}
\end{table}
\vfill
\begin{equation*}
h(\bx) = \sum_{j = 0}^{d} \theta_j x_j = \theta^T \bx
\end{equation*}
\pause
\vfill
Suppose we chose the following loss function:
\begin{equation*}
\ell \left( y, \yhat \right) = \dfrac{1}{2} \left( y - \yhat \right)^2
\end{equation*}
This leads to the following least squares \textit{cost function}:
\begin{equation*}
J(\theta) = \dfrac{1}{2} \sum_{i = 1}^{n} \left( h\left(\bxi\right) - \yi \right)^2
\end{equation*}
This problem the \textbf{ordinary least squares} (OLS) regression model.
\end{frame}

\begin{frame}{Least Mean Squares (LMS) update rule}
We want to minimize the following cost function:
\begin{equation*}
J(\theta) = \dfrac{1}{2} \sum_{i = 1}^{n} \left( h\left(\bxi\right) - \yi \right)^2
\end{equation*}
One way to do it is by using the \textbf{gradient descent} algorithm:
\begin{equation*}
\theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j} J(\theta)
\end{equation*}
for all $j \in \{ 0, \dots, d\}$. $\alpha$ is called the \textbf{step size} or the \textbf{learning rate}. This update rule can be rewritten in a more compact way:
\begin{equation*}
\theta := \theta - \alpha \nabla J(\theta)
\end{equation*}
where $\nabla J(\theta)$ is the \textbf{gradient} of $J$ in $\theta$. We have, by definition:
\begin{equation*}
\nabla J(\theta) = \left[ \dfrac{\partial}{\partial \theta_1} J(\theta), \dots, \dfrac{\partial}{\partial \theta_d} J(\theta) \right]^T
\end{equation*}
\end{frame}

\begin{frame}{Loss functions}
Recall we want to know whether $y$ a good prediction of $\yhat$.
\end{frame}

\begin{frame}{Regularization}

\end{frame}

\begin{frame}{Optimization}
Now, we want to minimize a function of the form
\begin{equation}
J(\theta) = \sum_{i = 1}^{n} \ell\left(\yi, \yhati\right) + \lambda R(\theta)
\end{equation}
\end{frame}

\begin{frame}{Conclusion}
Next week we will implement some of the concepts we've seen today.
\end{frame}

\begin{frame}
\vfill
\centering
\begin{huge}
\huge{Thank you! Questions?}
\vfill
\texttt{alexis.zubiolo@gmail.com}
\end{huge}
\vfill
\begin{Large}
\texttt{https://github.com/azubiolo/itstep}
\end{Large}
\vfill
\end{frame}

\end{document}
